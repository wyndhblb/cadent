

#############################
## System things
#############################

[system]

## PID file to check if things is running already
pid_file="/opt/cadent/all.pid"

## performance tuning suggest that n - 2 is best to avoid over creation
## of File Descriptors for the many sockets we need (and thus cause more OS
## overhead as time goes on)

num_procs=4

#############################
## Profiling
#############################

[profile]
## Turn on CPU/Mem profiling
enable=true

##  there will be a http server set up to yank pprof data on :6060

listen="0.0.0.0:6060"
rate=100000
block_profile=false # this is very expensive

#############################
## Internal Health/Stats
#############################

[health]

enabled=true

## there will be an internal health http server set up as well
## and will respond to '/ops/status' -> "GET OK"
## '/stats' --> blob of json data (adding ?jsonp=xxx will return jsonp data)
## '/' --> a little file that charts (poorly) the above stats
listen="0.0.0.0:6061"

## stats are "rendered" every 5 seconds, so this will keep last
## `internal_health_server_points` in RAM (careful with how many you store)
## (use the statsd emitter to store longer term stuff)
points=500

# https this server if desired
# key="/tmp/server.key"
# cert="/tmp/server.crt"


#############################
## Emit Internal To Statsd
#############################

[statsd]
## fire out some internal to statsd if desired
server="" # 127.0.0.1:8125"
prefix="cadent"
interval=1  # send to statd every second (buffered)

# global statsd Sample Rates

## It's HIGHLY recommended you at least put a statsd_timer_sample_rate as we measure
## rates of very fast functions, and it can take _alot_ of CPU cycles just to do that
## unless your server not under much stress 1% is a good number
## `statsd_sample_rate` is for counters
## `statsd_timer_sample_rate` is for timers
timer_sample_rate=0.01
sample_rate=0.1




#############################
## SERVERS
#############################
[servers]

#############################
## Incoming server defaults
#############################

[servers.default]

stats_tick=false


## cache keys/server pairs in an LRU mem cache for speed
cache_items=1000000

## Health Checking options
### check every "X" seconds
heartbeat_time_delay=60

### Timeout for failed connections (in Seconds)
heartbeat_time_timeout=1

## if a server has failed "X" times it's out of the loop
failed_heartbeat_count=3

## If we detect a downed node,
## `remove_node` -- we can REMOVE the node from the hashpool
## `ignore` -- stop checking the node
server_down_policy="ignore"

## outgoing connections are pooled per outgoign server
## this sets the number of pool connections
## 10 seems to be a "good" number for us
max_pool_connections=10

## if using the `bufferedpool` this is the size in bytes to use for the buffer
## (defaults to 512)
pool_buffersize=1024

## If using a UDP input, you may wish to set the buffer size for clients
## default is 1Mb
read_buffer_size=1048576
max_buffer_size=104857600

## number of workers to chew on the sending queue
## adjust this based on the input load
workers=16
out_workers=32

## there will be an internal health http server set up as well
## and will respond to '/ops/status' -> "GET OK"
## '/stats' --> blob of json data (adding ?jsonp=xxx will return jsonp data)
## '/' --> a little file that charts (poorly) the above stats
internal_health_server_listen="0.0.0.0:6062"

## stats are "rendered" every 5 seconds, so this will keep last
## `internal_health_server_points` in RAM (careful with how many you store)
## (use the statsd emitter to store longer term stuff)
internal_health_server_points=500


##
## Statsd server
##
[servers.statsd-proxy]

### Server sections .. note all the above is overridable in each section
### (except the `statsd_*` and `internal_health_server_listen`)
### NOTE: `hasher_replicas` will NOT use the defaults, be explict there

## what port/scheme we are listening on for incoming data
listen="udp://0.0.0.0:8125"

msg_type="statsd"

# statsd style
hasher_algo="mmh3"
hasher_elter="statsd"
hasher_vnodes=1

# all is prxied to the graphite item
out_dev_null=true


###
## Graphite Server
###

[servers.graphite-proxy]

### Server sections .. note all the above is overridable in each section
### (except the `statsd_*` and `internal_health_server_listen`)
### NOTE: `hasher_replicas` will NOT use the defaults, be explict there

## what port/scheme we are listening on for incoming data
listen="tcp://0.0.0.0:2003"

msg_type="graphite"

# graphite style
hasher_algo="mmh3"
hasher_elter="graphite"
hasher_vnodes=1

## need higher timeouts for graphite
write_timeout=5000
runner_timeout=5000
pool_buffersize=16384

#TCP buffers should be smaller as there are Many TCP conns
read_buffer_size=32768
max_buffer_size=819200
# just a stub to get going
out_dev_null=true