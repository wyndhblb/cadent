## this is the current config we run for Integ Stats collectors
## it acts like both Graphite and Statsd and flushes to graphite
##  the statsd out put is basically a loopback


#############################
## logging
#############################
log:
# format :  ""%{color}%{time:2006-01-02 15:04:05.000} [%{module}] (%{shortfile}) â–¶ %{level:.4s} %{color:reset} %{message}""
  level: "DEBUG"
  file: "stdout"


#############################
## System
#############################
system:

## PID file to check if things is running already
  pid_file: "/opt/cadent/cadent.pid"

## performance tuning suggest that n - 2 is best to avoid over creation
## of File Descriptors for the many sockets we need (and thus cause more OS
## overhead as time goes on)
#
  num_procs: 4

# if heap grows by this amount (%) trigger a GC
# same as GOGC env param
# if using FLAT writers at high volume (Elastic search and kafka in partcular) a good number is 80-100
# using series writers a good number is 60
# for direct flat writers (writers that do not use a write cache) (i.e. kafka) even larger is good
# as we can save lots of GC sweeps
  gc_percent: $ENV{GOGC:80}

# force a release of free ram ever "X" min
  free_mem_tick: $ENV{GOGC_FORCE_FREE_MEM:5}


#############################
## Profiling
#############################

profile:
## Turn on CPU/Mem profiling
  enabled: true

##  there will be a http server set up to yank pprof data on :6060

  listen: "0.0.0.0:6060"
  rate: 100000
  block_profile: false # this is very expensive

#############################
## Internal Stats
#############################

health:

  enabled: true

## there will be an internal health http server set up as well
## and will respond to '/ops/status' -> "GET OK"
## '/stats' --> blob of json data (adding ?jsonp: xxx will return jsonp data)
## '/' --> a little file that charts (poorly) the above stats
  listen: "0.0.0.0:6061"

## stats are "rendered" every 5 seconds, so this will keep last
## `internal_health_server_points` in RAM (careful with how many you store)
## (use the statsd emitter to store longer term stuff)
  points: 500

# https this server if desired
# key: "/tmp/server.key"
# cert: "/tmp/server.crt"


#############################
## Emit To Statsd
#############################

statsd:
## fire out some internal to statsd if desired
# server: "echo"  # use this just to echo to stdout (can be noisy)
# server: "tcp://host:port" if using a TCP statsd (default is UDP)
  server: "" #""127.0.0.1:8125"
  prefix: "cadent"
  interval: 1  # send to statd every second (buffered)

# global statsd Sample Rates

## It's HIGHLY recommended you at least put a statsd_timer_sample_rate as we measure
## rates of very fast functions, and it can take _alot_ of CPU cycles just to do that
## unless your server not under much stress 1% is a good number
## `statsd_sample_rate` is for counters
## `statsd_timer_sample_rate` is for timers
  timer_sample_rate: 0.01
  sample_rate: 0.1


#############################
## Gossiper
#############################
gossip:
  enabled: false
  mode: "lan"
  port: 8889


#############################
## SERVERS
#############################
servers:

  #############################
  ## Incoming server defaults
  #############################
  default:
      stats_tick: false


      ## cache keys/server pairs in an LRU mem cache for speed
      cache_items: 100000

      ## Health Checking options
      ### check every "X" seconds
      heartbeat_time_delay: 60

      ### Timeout for failed connections (in Seconds)
      heartbeat_time_timeout: 1

      ## if a server has failed "X" times it's out of the loop
      failed_heartbeat_count: 3

      ## If we detect a downed node,
      ## `remove_node` -- we can REMOVE the node from the hashpool
      ## `ignore` -- stop checking the node
      server_down_policy: "ignore"

      ## outgoing connections are pooled per outgoign server
      ## this sets the number of pool connections
      ## 10 seems to be a "good" number for us
      max_pool_connections: 10

      ## if using the `bufferedpool` this is the size in bytes to use for the buffer
      ## (defaults to 512)
      pool_buffersize: 1024

      ## If using a UDP input, you may wish to set the buffer size for clients
      ## default is 1Mb
      read_buffer_size: 1048576
      max_buffer_size: 104857600

      ## number of workers to chew on the sending queue
      ## adjust this based on the input load, dont make it too big otherwise you'll get stuck
      ## in the land of lock contententions
      workers: 10

      ## number of dispatcher workers to process output to backends
      out_workers: 32


  ################
  ## Statsd -> Backend Writer
  ################
  statsd-proxy:
      # internal statsd listener that pushes things into the below graphite section
      listen: "udp://0.0.0.0:8125"
      msg_type: "statsd"
      out_dev_null: true

  ################
  ## Graphite -> Backend Writer
  ################
  graphite-proxy:
      ## what port/scheme we are listening on for incoming data
      listen: "tcp://0.0.0.0:2003"
      workers: 5
      msg_type: "graphite"
      pool_buffersize: 16384

      # TCP buffers should be smaller then UDP as there can be Many TCP connnections
      read_buffer_size: 8192
      max_buffer_size: 819200

      # this implies NO backends, which is good if only writers are around or if each line gets re-routed to another backend
      # otherwise, you'd basically be doing nothing
      out_dev_null: true
